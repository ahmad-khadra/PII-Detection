{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 66653,
          "databundleVersionId": 7500999,
          "sourceType": "competition"
        },
        {
          "sourceId": 8218307,
          "sourceType": "datasetVersion",
          "datasetId": 4871495
        },
        {
          "sourceId": 8218320,
          "sourceType": "datasetVersion",
          "datasetId": 4871502
        },
        {
          "sourceId": 8218422,
          "sourceType": "datasetVersion",
          "datasetId": 4871568
        },
        {
          "sourceId": 8224247,
          "sourceType": "datasetVersion",
          "datasetId": 4876121
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import re\n",
        "from scipy.special import softmax\n",
        "from spacy.lang.en import English\n",
        "import os\n",
        "import regex"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-25T07:41:39.001570Z",
          "iopub.execute_input": "2024-04-25T07:41:39.001950Z",
          "iopub.status.idle": "2024-04-25T07:42:00.591593Z",
          "shell.execute_reply.started": "2024-04-25T07:41:39.001923Z",
          "shell.execute_reply": "2024-04-25T07:42:00.590738Z"
        },
        "trusted": true,
        "id": "Y1vv9UMbXgyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
        "\n",
        "ds = Dataset.from_dict({\n",
        "    \"full_text\": [x[\"full_text\"] for x in data],\n",
        "    \"document\": [x[\"document\"] for x in data],\n",
        "    \"tokens\": [x[\"tokens\"] for x in data],\n",
        "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
        "})"
      ],
      "metadata": {
        "id": "oZyHiBcsXgyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example, tokenizer):\n",
        "    text = []\n",
        "    token_map = []\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
        "\n",
        "        text.append(t)\n",
        "        token_map.extend([idx]*len(t))\n",
        "\n",
        "        if ws:\n",
        "            text.append(\" \")\n",
        "            token_map.append(-1)\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=3500)\n",
        "\n",
        "    return {\n",
        "        **tokenized,\n",
        "        \"token_map\": token_map,\n",
        "    }\n",
        "\n",
        "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
        "    idx = 0\n",
        "    spans = []\n",
        "    span = []\n",
        "\n",
        "    for i, token in enumerate(document):\n",
        "        if token != target[idx]:\n",
        "            idx = 0\n",
        "            span = []\n",
        "            continue\n",
        "        span.append(i)\n",
        "        idx += 1\n",
        "        if idx == len(target):\n",
        "            spans.append(span)\n",
        "            span = []\n",
        "            idx = 0\n",
        "            continue\n",
        "\n",
        "    return spans"
      ],
      "metadata": {
        "id": "S0TuAnTyXgyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = [\n",
        "    '/kaggle/input/deberta-large-lora',\n",
        "    '/kaggle/input/deberta-large2/deberta-large',\n",
        "    '/kaggle/input/deberta-base/deberta-base_2dataset_5e'\n",
        "]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_paths[0])\n",
        "\n",
        "# Tokenize the dataset using the 'tokenize' function in parallel\n",
        "ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc = 1)\n",
        "\n",
        "\n",
        "ensemble_preds = []\n",
        "model_weights = [6, 2.5, 1.5]\n",
        "total_weight = 5\n",
        "\n",
        "for model, w in zip(model_paths,model_weights):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of = 16)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model)\n",
        "    args = TrainingArguments(\n",
        "        \"/kaggle/working/output\",\n",
        "        report_to=\"none\",\n",
        "        do_train = False,\n",
        "        fp16 = True,\n",
        "        per_device_eval_batch_size=1\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    predictions = trainer.predict(ds).predictions\n",
        "    weighted_predictions = softmax(predictions, axis = -1) * w\n",
        "    ensemble_preds.append(weighted_predictions)\n",
        "\n",
        "weighted_average_predictions = np.sum(ensemble_preds, axis=0) / total_weight"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:45:26.791351Z",
          "iopub.execute_input": "2024-04-25T07:45:26.792045Z",
          "iopub.status.idle": "2024-04-25T07:45:39.741759Z",
          "shell.execute_reply.started": "2024-04-25T07:45:26.792010Z",
          "shell.execute_reply": "2024-04-25T07:45:39.740845Z"
        },
        "trusted": true,
        "id": "ubZ85M4OXgyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = json.load(open(\"/kaggle/input/deberta-large-lora/config.json\"))\n",
        "id2label = config[\"id2label\"]\n",
        "preds = weighted_average_predictions.argmax(-1)\n",
        "preds_no_O = weighted_average_predictions[:,:,:12].argmax(-1)\n",
        "only_O_preds = weighted_average_predictions[:,:,12]\n",
        "threshold = 0.97\n",
        "predictions_averaged = np.where(only_O_preds < threshold, preds_no_O , preds)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:46:00.064605Z",
          "iopub.execute_input": "2024-04-25T07:46:00.065547Z",
          "iopub.status.idle": "2024-04-25T07:46:00.411932Z",
          "shell.execute_reply.started": "2024-04-25T07:46:00.065489Z",
          "shell.execute_reply": "2024-04-25T07:46:00.411001Z"
        },
        "trusted": true,
        "id": "dFXcJF5RXgyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = set()  # membership operation using set is faster O(1) than that of list O(n)\n",
        "processed = []\n",
        "emails = []\n",
        "phone_nums = []\n",
        "urls = []\n",
        "streets = []\n",
        "\n",
        "# For each prediction, token mapping, offsets, tokens, and document in the dataset\n",
        "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
        "    preds_final,\n",
        "    ds[\"token_map\"],\n",
        "    ds[\"offset_mapping\"],\n",
        "    ds[\"tokens\"],\n",
        "    ds[\"document\"],\n",
        "    ds[\"full_text\"]\n",
        "):\n",
        "\n",
        "    # Iterate through each token prediction and its corresponding offsets\n",
        "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
        "        label_pred = id2label[str(token_pred)]  # Predicted label from token\n",
        "        if start_idx + end_idx == 0:\n",
        "            continue\n",
        "        if token_map[start_idx] == -1:\n",
        "            start_idx += 1\n",
        "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
        "            start_idx += 1\n",
        "        if start_idx >= len(token_map):\n",
        "            break\n",
        "        token_id = token_map[start_idx]  # Token ID at start index\n",
        "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
        "            continue\n",
        "\n",
        "        pair = (doc, token_id)\n",
        "        if pair not in pairs:\n",
        "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
        "            pairs.add(pair)\n",
        "\n",
        "    # email\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        if re.fullmatch(email_regex, token) is not None:\n",
        "            emails.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
        "            )\n",
        "\n",
        "    # phone number\n",
        "    matches = phone_num_regex.findall(full_text)\n",
        "    if not matches:\n",
        "        continue\n",
        "    for match in matches:\n",
        "        target = [t.text for t in nlp.tokenizer(match)]\n",
        "        matched_spans = find_span(target, tokens)\n",
        "    for matched_span in matched_spans:\n",
        "        for intermediate, token_idx in enumerate(matched_span):\n",
        "            prefix = \"I\" if intermediate else \"B\"\n",
        "            phone_nums.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
        "            )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:46:33.881906Z",
          "iopub.execute_input": "2024-04-25T07:46:33.882668Z",
          "iopub.status.idle": "2024-04-25T07:46:33.998311Z",
          "shell.execute_reply.started": "2024-04-25T07:46:33.882627Z",
          "shell.execute_reply": "2024-04-25T07:46:33.997386Z"
        },
        "trusted": true,
        "id": "_sCjYYiBXgyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix BIO Format\n",
        "for i in range(len(processed)):\n",
        "    if i == 0:\n",
        "        temp = processed[i]['label']\n",
        "        processed[i]['label'] = 'B-' + processed[i]['label'][2:]\n",
        "    else:\n",
        "        previous = processed[i-1]['token']\n",
        "        current = processed[i]['token'] - 1\n",
        "        if temp[2:] == processed[i]['label'][2:] and previous == current and processed[i]['document'] == processed[i-1]['document']:\n",
        "            temp = processed[i]['label']\n",
        "            processed[i]['label'] = 'I-' + processed[i]['label'][2:]\n",
        "        else:\n",
        "            temp = processed[i]['label']\n",
        "            processed[i]['label'] = 'B-' + processed[i]['label'][2:]\n",
        "\n",
        "import regex\n",
        "i = 0\n",
        "delete_count = 0\n",
        "while i < len(processed):\n",
        "    doc_idx = processed[i]['document']\n",
        "\n",
        "    if bool(re.search(r'^B-', processed[i]['label'])):\n",
        "\n",
        "        is_next_token = False\n",
        "        if i+1 < len(processed):\n",
        "            if bool(re.search(r'^I-', processed[i+1]['label'])):\n",
        "                is_next_token = True\n",
        "        if not(is_next_token):\n",
        "\n",
        "            # B-ID_NUM\n",
        "            if (processed[i]['label']  == 'B-ID_NUM'):\n",
        "                if not(bool(re.search(r'\\d{2}', processed[i]['token_str']))) or len(processed[i]['token_str']) <=3:\n",
        "                    print(processed[i])\n",
        "                    del processed[i]\n",
        "                    delete_count += 1\n",
        "                    continue\n",
        "\n",
        "            # B-STREET_ADDRESS\n",
        "#             if (processed[i]['label']  == 'B-STREET_ADDRESS'):\n",
        "#                 if len(processed[i]['token_str']) <=11:\n",
        "#                     print(processed[i])\n",
        "#                     del processed[i]\n",
        "#                     delete_count += 1\n",
        "#                     continue\n",
        "\n",
        "            # B-NAME_STUDENT\n",
        "            if (processed[i]['label']  == 'B-NAME_STUDENT'):\n",
        "                if len(processed[i]['token_str']) <=1: #Because there are cases where \".\" is labeled as NAME_STUDENT!\n",
        "                    print(processed[i])\n",
        "                    del processed[i]\n",
        "                    delete_count += 1\n",
        "                    continue\n",
        "\n",
        "            # B-PHONE_NUM\n",
        "            if (processed[i]['label']  == 'B-PHONE_NUM'):\n",
        "                if len(processed[i]['token_str']) <=4:\n",
        "                    print(processed[i])\n",
        "                    del processed[i]\n",
        "                    delete_count += 1\n",
        "                    continue\n",
        "\n",
        "            # B-EMAIL\n",
        "            if (processed[i]['label']  == 'B-EMAIL'):\n",
        "                if not(bool(re.search(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', processed[i]['token_str']))):\n",
        "                    print(processed[i])\n",
        "                    del processed[i]\n",
        "                    delete_count += 1\n",
        "                    continue\n",
        "\n",
        "\n",
        "    if processed[i]['label']  == 'B-NAME_STUDENT':\n",
        "        if not(bool(regex.match(r'^\\p{Lu}\\p{Ll}+$', processed[i]['token_str']))):\n",
        "            print(processed[i])\n",
        "            del processed[i]\n",
        "            delete_count += 1\n",
        "            continue\n",
        "\n",
        "    if processed[i]['label']  == 'I-NAME_STUDENT':\n",
        "        if not(bool(re.search(r'^[A-Z][a-z\\.]+$', processed[i]['token_str']))):\n",
        "            print(processed[i])\n",
        "            del processed[i]\n",
        "            delete_count += 1\n",
        "            continue\n",
        "\n",
        "    if processed[i]['label']  == 'B-URL_PERSONAL':\n",
        "        if len(processed[i]['token_str']) <=1:\n",
        "            print(processed[i])\n",
        "            del processed[i]\n",
        "            delete_count += 1\n",
        "            continue\n",
        "    i+=1\n",
        "\n",
        "# Fix BIO Format AGAIN\n",
        "for i in range(len(processed)):\n",
        "    if i == 0:\n",
        "        temp = processed[i]['label']\n",
        "        processed[i]['label'] = 'B-' + processed[i]['label'][2:]\n",
        "    else:\n",
        "        previous = processed[i-1]['token']\n",
        "        current = processed[i]['token'] - 1\n",
        "        if temp[2:] == processed[i]['label'][2:] and previous == current and processed[i]['document'] == processed[i-1]['document']:\n",
        "            temp = processed[i]['label']\n",
        "            processed[i]['label'] = 'I-' + processed[i]['label'][2:]\n",
        "        else:\n",
        "            temp = processed[i]['label']\n",
        "            processed[i]['label'] = 'B-' + processed[i]['label'][2:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-25T07:46:36.500418Z",
          "iopub.execute_input": "2024-04-25T07:46:36.501334Z",
          "iopub.status.idle": "2024-04-25T07:46:36.521416Z",
          "shell.execute_reply.started": "2024-04-25T07:46:36.501295Z",
          "shell.execute_reply": "2024-04-25T07:46:36.520519Z"
        },
        "trusted": true,
        "id": "WXtz_vvhXgyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(processed + phone_nums + emails + urls)\n",
        "\n",
        "# Assign each row a unique 'row_id'\n",
        "df[\"row_id\"] = list(range(len(df)))\n",
        "\n",
        "# Cast your findings into a CSV file for further exploration\n",
        "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "3OqvMdAZXgyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: The training notebooks are still the same as the previous milestone"
      ],
      "metadata": {
        "id": "CFR70G3Q9HtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wpW5Yshq9PDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}