{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Please note that this session has crashed due to running out of GPU. so these parameters were supposed to train a third model."
      ],
      "metadata": {
        "id": "7gwGg1GOBkzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOCMBz2hHagy",
        "outputId": "47b3e2e5-88c7-49e6-a531-0136fb342331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q peft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gOQAgcmwBjho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcbQvP-9WqmS",
        "outputId": "bf3d390f-7898-4295-85b7-4e02045bf974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61bbgb5PIBFv",
        "outputId": "7a4bcf01-2b9c-4192-ba1e-c624551cc1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrR1zx3EIEta",
        "outputId": "c13222e8-1ce5-4b8b-deff-69148817fbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import copy\n",
        "import gc\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from spacy.lang.en import English\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
        "from transformers.models.deberta_v2 import DebertaV2ForTokenClassification, DebertaV2TokenizerFast\n",
        "from transformers.trainer import Trainer\n",
        "from transformers.training_args import TrainingArguments\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
        "from peft.mapping import get_peft_config, get_peft_model\n",
        "from peft.peft_model import PeftModel\n",
        "from peft.config import PeftConfig\n",
        "from peft.tuners.lora import LoraConfig\n",
        "from peft.utils import TaskType\n",
        "from datasets import Dataset, DatasetDict, concatenate_datasets"
      ],
      "metadata": {
        "id": "72DKaeogHeRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\n",
        "TRAINING_MAX_LENGTH = 1024\n",
        "EVAL_MAX_LENGTH = 3072\n",
        "CONF_THRESH = 0.9\n",
        "LR = 1.3e-4\n",
        "LR_SCHEDULER_TYPE = \"linear\"\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 1\n",
        "EVAL_BATCH_SIZE = 1\n",
        "GRAD_ACCUMULATION_STEPS = 16 // BATCH_SIZE\n",
        "WARMUP_RATIO = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "FREEZE_EMBEDDING = False\n",
        "FREEZE_LAYERS = 6\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = LORA_R * 2\n",
        "AMP = True\n",
        "N_SPLITS = 3\n",
        "NEGATIVE_RATIO = 1\n",
        "OUTPUT_DIR = \"output\"\n",
        "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "bJVTJbFdVOOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir='/content/output',\n",
        "    fp16=True,\n",
        "    learning_rate=5e-04,\n",
        "    num_train_epochs=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    report_to=\"none\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    do_eval = True,\n",
        "    eval_steps=500,\n",
        "    eval_delay=0,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    metric_for_best_model=\"f5\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "#     overwrite_output_dir=True,\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "HAqN5vj3HmaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_modules = [\"key_proj\", \"value_proj\", \"query_proj\", \"dense\", \"classifier\"]\n",
        "    target_modules.append(\"word_embeddings\")\n",
        "    lora_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,\n",
        "    inference_mode=False,\n",
        "    target_modules=target_modules,\n",
        "    modules_to_save=[\"classifier\"],\n",
        "    layers_to_transform=[i for i in range(6, 24)],\n",
        "    r=16,\n",
        "    lora_alpha= 32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"lora_only\"\n",
        ")"
      ],
      "metadata": {
        "id": "dS8y3zTWIRiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_data = json.load(open('/content/drive/MyDrive/PII/Copy of train.json'))\n",
        "extra_data = json.load(open('/content/drive/MyDrive/PII/Copy of mpware_mixtral8x7b_v1.1-no-i-username.json'))\n",
        "data = original_data + extra_data"
      ],
      "metadata": {
        "id": "xdsPXT86I48x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = [\n",
        "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n",
        "]\n",
        "id2label = {i: l for i, l in enumerate(all_labels)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "target = [l for l in all_labels if l != \"O\"]"
      ],
      "metadata": {
        "id": "ZsgQVdr3Jajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, label2id: dict, max_length: int) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, example: dict) -> dict:\n",
        "        # rebuild text from tokens\n",
        "        text, labels, token_map = [], [], []\n",
        "\n",
        "        for idx, (t, l, ws) in enumerate(\n",
        "            zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"])\n",
        "        ):\n",
        "            text.append(t)\n",
        "            labels.extend([l] * len(t))\n",
        "            token_map.extend([idx]*len(t))\n",
        "\n",
        "            if ws:\n",
        "                text.append(\" \")\n",
        "                labels.append(\"O\")\n",
        "                token_map.append(-1)\n",
        "\n",
        "        text = \"\".join(text)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # actual tokenization\n",
        "        tokenized = self.tokenizer(\n",
        "            \"\".join(text),\n",
        "            return_offsets_mapping=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        token_labels = []\n",
        "\n",
        "        for start_idx, end_idx in tokenized.offset_mapping:\n",
        "            # CLS token\n",
        "            if start_idx == 0 and end_idx == 0:\n",
        "                token_labels.append(self.label2id[\"O\"])\n",
        "                continue\n",
        "\n",
        "            # case when token starts with whitespace\n",
        "            if text[start_idx].isspace():\n",
        "                start_idx += 1\n",
        "\n",
        "            token_labels.append(self.label2id[labels[start_idx]])\n",
        "\n",
        "        length = len(tokenized.input_ids)\n",
        "\n",
        "        return {**tokenized, \"labels\": token_labels, \"length\": length, \"token_map\": token_map}"
      ],
      "metadata": {
        "id": "bLFG07n0Jct-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DebertaV2TokenizerFast.from_pretrained(TRAINING_MODEL_PATH)\n",
        "train_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=TRAINING_MAX_LENGTH)\n",
        "eval_encoder = CustomTokenizer(tokenizer=tokenizer, label2id=label2id, max_length=EVAL_MAX_LENGTH)\n",
        "\n",
        "ds = DatasetDict()\n",
        "\n",
        "for key, data in zip([\"original\", \"extra\"], [original_data, extra_data]):\n",
        "    ds[key] = Dataset.from_dict({\n",
        "        \"full_text\": [x[\"full_text\"] for x in data],\n",
        "        \"document\": [str(x[\"document\"]) for x in data],\n",
        "        \"tokens\": [x[\"tokens\"] for x in data],\n",
        "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
        "        \"provided_labels\": [x[\"labels\"] for x in data],\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVYKOdtPJ4bR",
        "outputId": "dfa4769b-35a7-4060-d89b-fef9bc059405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
        "    idx = 0\n",
        "    spans = []\n",
        "    span = []\n",
        "\n",
        "    for i, token in enumerate(document):\n",
        "        if token != target[idx]:\n",
        "            idx = 0\n",
        "            span = []\n",
        "            continue\n",
        "        span.append(i)\n",
        "        idx += 1\n",
        "        if idx == len(target):\n",
        "            spans.append(span)\n",
        "            span = []\n",
        "            idx = 0\n",
        "            continue\n",
        "\n",
        "    return spans\n",
        "\n",
        "\n",
        "class PRFScore:\n",
        "    \"\"\"A precision / recall / F score.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        tp: int = 0,\n",
        "        fp: int = 0,\n",
        "        fn: int = 0,\n",
        "    ) -> None:\n",
        "        self.tp = tp\n",
        "        self.fp = fp\n",
        "        self.fn = fn\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.tp + self.fp + self.fn\n",
        "\n",
        "    def __iadd__(self, other):  # in-place add\n",
        "        self.tp += other.tp\n",
        "        self.fp += other.fp\n",
        "        self.fn += other.fn\n",
        "        return self\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return PRFScore(\n",
        "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
        "        )\n",
        "\n",
        "    def score_set(self, cand: set, gold: set) -> None:\n",
        "        self.tp += len(cand.intersection(gold))\n",
        "        self.fp += len(cand - gold)\n",
        "        self.fn += len(gold - cand)\n",
        "\n",
        "    @property\n",
        "    def precision(self) -> float:\n",
        "        return self.tp / (self.tp + self.fp + 1e-100)\n",
        "\n",
        "    @property\n",
        "    def recall(self) -> float:\n",
        "        return self.tp / (self.tp + self.fn + 1e-100)\n",
        "\n",
        "    @property\n",
        "    def f1(self) -> float:\n",
        "        p = self.precision\n",
        "        r = self.recall\n",
        "        return 2 * ((p * r) / (p + r + 1e-100))\n",
        "\n",
        "    @property\n",
        "    def f5(self) -> float:\n",
        "        beta = 5\n",
        "        p = self.precision\n",
        "        r = self.recall\n",
        "\n",
        "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
        "        return fbeta\n",
        "\n",
        "    def to_dict(self) -> dict[str, float]:\n",
        "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}"
      ],
      "metadata": {
        "id": "IuKGzystLiGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsComputer:\n",
        "    nlp = English()\n",
        "\n",
        "    def __init__(self, eval_ds: Dataset, label2id: dict, conf_thresh: float = 0.9) -> None:\n",
        "        self.ds = eval_ds.remove_columns(\"labels\").rename_columns({\"provided_labels\": \"labels\"})\n",
        "        self.gt_df = self.create_gt_df(self.ds)\n",
        "        self.label2id = label2id\n",
        "        self.confth = conf_thresh\n",
        "        self._search_gt()\n",
        "\n",
        "    def __call__(self, eval_preds: EvalPrediction) -> dict:\n",
        "        pred_df = self.create_pred_df(eval_preds.predictions)\n",
        "        return self.compute_metrics_from_df(self.gt_df, pred_df)\n",
        "\n",
        "    def _search_gt(self) -> None:\n",
        "        email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "        phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
        "        self.emails = []\n",
        "        self.phone_nums = []\n",
        "\n",
        "        for _data in self.ds:\n",
        "            # email\n",
        "            for token_idx, token in enumerate(_data[\"tokens\"]):\n",
        "                if re.fullmatch(email_regex, token) is not None:\n",
        "                    self.emails.append(\n",
        "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
        "                    )\n",
        "            # phone number\n",
        "            matches = phone_num_regex.findall(_data[\"full_text\"])\n",
        "            if not matches:\n",
        "                continue\n",
        "            for match in matches:\n",
        "                target = [t.text for t in self.nlp.tokenizer(match)]\n",
        "                matched_spans = find_span(target, _data[\"tokens\"])\n",
        "            for matched_span in matched_spans:\n",
        "                for intermediate, token_idx in enumerate(matched_span):\n",
        "                    prefix = \"I\" if intermediate else \"B\"\n",
        "                    self.phone_nums.append(\n",
        "                        {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
        "                    )\n",
        "\n",
        "    @staticmethod\n",
        "    def create_gt_df(ds: Dataset):\n",
        "        gt = []\n",
        "        for row in ds:\n",
        "            for token_idx, (token, label) in enumerate(zip(row[\"tokens\"], row[\"labels\"])):\n",
        "                if label == \"O\":\n",
        "                    continue\n",
        "                gt.append(\n",
        "                    {\"document\": row[\"document\"], \"token\": token_idx, \"label\": label, \"token_str\": token}\n",
        "                )\n",
        "        gt_df = pd.DataFrame(gt)\n",
        "        gt_df[\"row_id\"] = gt_df.index\n",
        "\n",
        "        return gt_df\n",
        "\n",
        "    def create_pred_df(self, logits: np.ndarray) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Note:\n",
        "            Thresholing is doen on logits instead of softmax, which could find better models on LB.\n",
        "        \"\"\"\n",
        "        prediction = logits\n",
        "        o_index = self.label2id[\"O\"]\n",
        "        preds = prediction.argmax(-1)\n",
        "        preds_without_o = prediction.copy()\n",
        "        preds_without_o[:,:,o_index] = 0\n",
        "        preds_without_o = preds_without_o.argmax(-1)\n",
        "        o_preds = prediction[:,:,o_index]\n",
        "        preds_final = np.where(o_preds < self.confth, preds_without_o , preds)\n",
        "\n",
        "        pairs = set()\n",
        "        processed = []\n",
        "\n",
        "        # Iterate over document\n",
        "        for p_doc, token_map, offsets, tokens, doc in zip(\n",
        "            preds_final, self.ds[\"token_map\"], self.ds[\"offset_mapping\"], self.ds[\"tokens\"], self.ds[\"document\"]\n",
        "        ):\n",
        "            # Iterate over sequence\n",
        "            for p_token, (start_idx, end_idx) in zip(p_doc, offsets):\n",
        "                label_pred = id2label[p_token]\n",
        "\n",
        "                if start_idx + end_idx == 0:\n",
        "                    # [CLS] token i.e. BOS\n",
        "                    continue\n",
        "\n",
        "                if token_map[start_idx] == -1:\n",
        "                    start_idx += 1\n",
        "\n",
        "                # ignore \"\\n\\n\"\n",
        "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
        "                    start_idx += 1\n",
        "\n",
        "                if start_idx >= len(token_map):\n",
        "                    break\n",
        "\n",
        "                token_id = token_map[start_idx]\n",
        "                pair = (doc, token_id)\n",
        "\n",
        "                # ignore \"O\", preds, phone number and  email\n",
        "                if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
        "                    continue\n",
        "\n",
        "                if pair in pairs:\n",
        "                    continue\n",
        "\n",
        "                processed.append(\n",
        "                    {\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]}\n",
        "                )\n",
        "                pairs.add(pair)\n",
        "\n",
        "        pred_df = pd.DataFrame(processed + self.emails + self.phone_nums)\n",
        "        pred_df[\"row_id\"] = list(range(len(pred_df)))\n",
        "\n",
        "        return pred_df\n",
        "\n",
        "    def compute_metrics_from_df(self, gt_df, pred_df):\n",
        "        \"\"\"\n",
        "        Compute the LB metric (lb) and other auxiliary metrics\n",
        "        \"\"\"\n",
        "\n",
        "        references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
        "        predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
        "\n",
        "        score_per_type = defaultdict(PRFScore)\n",
        "        references = set(references)\n",
        "\n",
        "        for ex in predictions:\n",
        "            pred_type = ex[-1] # (document, token, label)\n",
        "            if pred_type != 'O':\n",
        "                pred_type = pred_type[2:] # avoid B- and I- prefix\n",
        "\n",
        "            if pred_type not in score_per_type:\n",
        "                score_per_type[pred_type] = PRFScore()\n",
        "\n",
        "            if ex in references:\n",
        "                score_per_type[pred_type].tp += 1\n",
        "                references.remove(ex)\n",
        "            else:\n",
        "                score_per_type[pred_type].fp += 1\n",
        "\n",
        "        for doc, tok, ref_type in references:\n",
        "            if ref_type != 'O':\n",
        "                ref_type = ref_type[2:] # avoid B- and I- prefix\n",
        "\n",
        "            if ref_type not in score_per_type:\n",
        "                score_per_type[ref_type] = PRFScore()\n",
        "            score_per_type[ref_type].fn += 1\n",
        "\n",
        "        totals = PRFScore()\n",
        "\n",
        "        for prf in score_per_type.values():\n",
        "            totals += prf\n",
        "\n",
        "        return {\n",
        "            \"precision\": totals.precision,\n",
        "            \"recall\": totals.recall,\n",
        "            \"f5\": totals.f5,\n",
        "            **{\n",
        "                f\"{v_k}-{k}\": v_v\n",
        "                for k in set([l[2:] for l in self.label2id.keys() if l!= 'O'])\n",
        "                for v_k, v_v in score_per_type[k].to_dict().items()\n",
        "            },\n",
        "        }"
      ],
      "metadata": {
        "id": "i_IpGr1tKaUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelInit:\n",
        "    def __init__(\n",
        "        self,\n",
        "        checkpoint: str,\n",
        "        id2label: dict,\n",
        "        label2id: dict,\n",
        "        peft_config: PeftConfig,\n",
        "    ) -> None:\n",
        "        self.checkpoint = checkpoint\n",
        "        self.id2label = id2label\n",
        "        self.label2id = label2id\n",
        "        self.peft_config = peft_config\n",
        "\n",
        "    def __call__(self) -> DebertaV2ForTokenClassification:\n",
        "        base_model = DebertaV2ForTokenClassification.from_pretrained(\n",
        "            self.checkpoint,\n",
        "            num_labels=len(self.id2label),\n",
        "            id2label=self.id2label,\n",
        "            label2id=self.label2id,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model = get_peft_model(base_model, self.peft_config)\n",
        "        model.print_trainable_parameters()\n",
        "        return model\n",
        "\n",
        "model_init = ModelInit(\n",
        "    TRAINING_MODEL_PATH,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    peft_config=lora_config\n",
        ")"
      ],
      "metadata": {
        "id": "cAgqJhnlU_cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_ds = ds[\"original\"]\n",
        "train_ds = concatenate_datasets([original_ds, ds['extra']])\n",
        "train_ds = train_ds.map(train_encoder, num_proc=os.cpu_count())\n",
        "eval_ds = ds[\"original\"].select(eval_idx)\n",
        "eval_ds = eval_ds.map(eval_encoder, num_proc=os.cpu_count())\n",
        "trainer = Trainer(\n",
        "    args=args,\n",
        "    model_init=model_init,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=MetricsComputer(eval_ds=eval_ds, label2id=label2id),\n",
        "    data_collator=DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16),\n",
        ")\n",
        "trainer.train()\n",
        "eval_res = trainer.evaluate(eval_dataset=eval_ds)\n",
        "with open(os.path.join(args.output_dir, \"eval_result.json\"), \"w\") as f:\n",
        "    json.dump(eval_res, f)\n",
        "trainer.model = trainer.model.base_model.merge_and_unload()\n",
        "trainer.save_model(os.path.join(OUTPUT_DIR, f\"fold_{fold_idx}\", \"best\"))\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "break"
      ],
      "metadata": {
        "id": "BcM4Ha8DVWVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXuZu8OGXkOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}